{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purnachand1186/LLM_Engineering/blob/main/Diagnose_Llama_Access_issues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diagnose common problems with HuggingFace access\n",
        "\n",
        "Please step through this colab to figure out the issue.\n",
        "\n",
        "By this point, you should have pressed the Key symbol in the sidebar to the left, and entered a secret with:  \n",
        "\n",
        "Key: `HF_TOKEN`  \n",
        "Value: your actual token, starting hf_.\n",
        "\n",
        "And the switch next to it should be turned on.\n",
        "\n",
        "See bottom of this colab if you've not been approved for Meta."
      ],
      "metadata": {
        "id": "JlKvxCUfIemW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "fKcu6GFAHS5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token.startswith(\"hf_\"):\n",
        "  print(f\"HF Token has the right prefix and begins -->{hf_token[:5]}<--\")\n",
        "else:\n",
        "  print(\"HF Token not found - please use the key icon in the left sidebar to add your HF Token as a secret\")"
      ],
      "metadata": {
        "id": "Bxf538EAHk7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpw8fHhWHRiu"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "def diagnose_huggingface_error(hf_token):\n",
        "    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
        "\n",
        "    # Step 1: Check if the token is valid\n",
        "    response = requests.get(\"https://huggingface.co/api/whoami-v2\", headers=headers)\n",
        "    if response.status_code == 401:\n",
        "        return \"Error: Invalid HuggingFace token. Please check if you are properly signed in and your token is correct.\"\n",
        "\n",
        "    # Step 2: Check if you've been accepted by Meta - see instructions in last cell if not\n",
        "    model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "    response = requests.head(f\"https://huggingface.co/{model_id}\", headers=headers)\n",
        "    if response.status_code == 403:\n",
        "        return (\"Error: Access denied to the model. \"\n",
        "                \"Please ensure you have agreed to Meta's terms for the Llama model. \"\n",
        "                \"Visit: https://huggingface.co/meta-llama\")\n",
        "\n",
        "    # Step 3: Check fine-grained permissions of the token\n",
        "    api = HfApi()\n",
        "    token_info = api.whoami(token=hf_token)\n",
        "    if \"scope\" in token_info:\n",
        "        scopes = token_info[\"scope\"]\n",
        "        if \"read\" not in scopes:\n",
        "            return (\"Error: Token does not have read access to external repos. \"\n",
        "                    \"Ensure the token's fine-grained permissions include 'read'.\")\n",
        "\n",
        "    # Step 4: Test if model loading is allowed\n",
        "    try:\n",
        "        AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=hf_token)\n",
        "        return \"Success: All checks passed, and the model should load correctly.\"\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error when loading the model: {e}\"\n",
        "\n",
        "result = diagnose_huggingface_error(hf_token)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If Meta has not approved your access to Llama 3.1\n",
        "\n",
        "It's a mystery to me, but for some reason, occasionally Meta doesn't approve access. They don't give a reason.\n",
        "\n",
        "If this happens to you, you can use Qwen 2.5 (or Qwen 3) as a swap-in replacement to Llama throughout this course!\n",
        "\n",
        "But if you'd like to persist with Llama, then the following steps have worked with every student I've spoken with so far. Please let me know if this doesn't work for you!\n",
        "\n",
        "\n",
        "1. Create a new gmail email (ugh) or use a spare if you have one\n",
        "2. Create a new HF account via this new gmail and create a new WRITE token for your `.env` file and colab secrets\n",
        "3. Apply for Llama 3.1 access (only 3.1, not 3.2)\n",
        "4. For email, be sure to use this identical new gmail email address - be absolutely sure that the email matches precisely\n",
        "5. For Organization Affiliation put `<Your Name> Personal Education` and for role put `AI Researcher`\n",
        "\n",
        "Let me know! And remember, Qwen is just as good - if not better.\n"
      ],
      "metadata": {
        "id": "g2_89TT7jWTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A last resort! Submitting to Meta\n",
        "\n",
        "A student (thank you Sergio!) has suggested this approach as a final last resort, which was successful for him:\n",
        "\n",
        "___\n",
        "\n",
        "If someone's access has been denied, here is a suggested alternative: go directly to the Meta webpage and request the model from there:\n",
        "\n",
        "https://www.llama.com/llama-downloads/\n",
        "\n",
        "1. Fill out the requested information and introduce an Organization/Affiliation. In my case, I used the name of my previous company. Students could perhaps use the name of their university or school.\n",
        "2. Select the Llama model(s).\n",
        "3. Once access granted, follow the instructions.\n",
        "\n",
        "It is likely that users will need to replace this line:\n",
        "\n",
        "`tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)`\n",
        "\n",
        "with the following:\n",
        "\n",
        "```\n",
        "!pip install llama-stack\n",
        "!llama model list\n",
        "!llama model download --source meta --model-id Â Llama3.1-8B\n",
        "tokenizer = AutoTokenizer.from_pretrained('/root/.llama/checkpoints/Llama3.1-8B', trust_remote_code=True)\n",
        "```\n",
        "\n",
        "Meta will likely provide a link that the user must paste during the download/authentication process.\n"
      ],
      "metadata": {
        "id": "2MaV7J5GgedO"
      }
    }
  ]
}